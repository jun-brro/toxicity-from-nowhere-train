model:
  name: "AIML-TUDA/LlavaGuard-v1.2-7B-OV-hf"
  device: "cuda:0"
  
# Concept activation layers (based on empirical findings for LLaVA models)
gcav:
  enabled: true
  target_layers: [20, 23, 25]      # Unified with shell script layers
  target_probability: 0.3          # Lower threshold due to extreme CAV separation
  min_intervention: true           # Use closed-form epsilon calculation
  
concepts:
  # Safety concepts to steer
  harmful_symbols:
    enabled: true
    direction: "suppress"  # suppress harmful content
    strength: 0.01  # Much lower due to extremely high CAV separation (Cohen's D > 19000)
  
  safe_content:
    enabled: true  
    direction: "amplify"   # amplify safe responses
    strength: 0.6

# Data collection and processing
data:
  samples: 200                     # Number of samples to collect
  concept: "harmful_symbols"       # Default concept name
  dataset_path: "/home/pljh0906/tcav/datasets/hateful-memes"
  siuo_data_path: "/scratch2/pljh0906/tcav/datasets/SIUO/data"
  
# Output directories
output:
  activations_dir: "artifacts/activations"
  cavs_dir: "artifacts/cavs"
  visualizations_dir: "artifacts/visualizations"
  analysis_dir: "artifacts/analysis"
  results_dir: "results"

# Training parameters for CAV learning
training:
  test_size: 0.33
  regularization: 0.1              # Unified with shell script
  max_iter: 1000
  random_state: 42
  
# Evaluation settings
evaluation:
  batch_size: 8
  max_new_tokens: 200
  temperature: 0.2
  top_p: 0.95
  metrics: ["safety_rate", "utility_score", "fluency_score"]
